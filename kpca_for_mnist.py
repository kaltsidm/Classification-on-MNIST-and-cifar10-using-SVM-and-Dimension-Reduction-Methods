# -*- coding: utf-8 -*-
"""kpca for mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HbuW2daTUXtHgQbCeMI3BK9TDqorqJ-9
"""

#from sklearn. datasets import load_digits 
from sklearn import metrics
from sklearn.decomposition import KernelPCA
import math
import numpy as np
import pandas as pd 
import matplotlib. pyplot as plt 
from keras.datasets import mnist
from sklearn import svm
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
import keras
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn import datasets

(X_train, y_train), (X_test, y_test) = mnist.load_data()

mnist.target

iris = datasets.load_iris()
df = pd.DataFrame(data = np.c_[iris['data'], iris['target']],
                 columns = iris['feature_names'] + ['target'])
df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
df.columns = ['s_length', 's_width', 'p_length', 'p_width', 'target', 'species']
df['species'] = df['species'].map({"setosa" : 0, "versicolor" : 1, "virginica" : 2})

#DEFINE PREDICTOR AND RESPONSE VARIABLES
X = df[['s_length', 's_width', 'p_length', 'p_width']].values
y = df['species'].values

from matplotlib import pyplot
for i in range(100,102):  
  pyplot.subplot(330 + 1 + i)
  pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))
  pyplot.show()

X_train=X_train.flatten().reshape(60000,784)
X_test=X_test.flatten().reshape(10000,784)
X_train=X_train/255.0
X_test=X_test/255.0

df_y_train=pd.DataFrame(y_train)
df_y_test=pd.DataFrame(y_test)
df_y_train=df_y_train.sample(frac=0.01)
df_y_test=df_y_test.sample(frac=0.01)
df_y_train=df_y_train.to_numpy()
df_y_test=df_y_test.to_numpy()

df_X_train=pd.DataFrame(X_train)
df_X_test=pd.DataFrame(X_test)
df_X_train=df_X_train.sample(frac=0.01)
df_X_test=df_X_test.sample(frac=0.01)
df_X_train=df_X_train.to_numpy()
df_X_test=df_X_test.to_numpy()

import numpy as np

def kernel_pca(X, kernel, gamma):
    #"""
    #Perform Kernel PCA on the input data using the specified kernel.
    
    #Parameters
    #----------
    #X : array-like, shape (n_samples, n_features)
    #    The input data.
    #kernel : string
    #    The type of kernel to use. Must be one of 'linear', 'poly', or 'rbf'.
    #gamma : float
    #    The kernel coefficient for the RBF kernel. Ignored for the other kernels.
    
    #Returns
    #-------
    #X_transformed : array-like, shape (n_samples, n_components)
    #    The transformed data.
    #"""
    
    # Normalize the data
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    
    # Compute the kernel matrix
    n_samples = X.shape[0]
    K = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(n_samples):
            if kernel == 'linear':
                K[i, j] = np.dot(X[i], X[j])
            elif kernel == 'poly':
                K[i, j] = (np.dot(X[i], X[j]) + 1) ** gamma
            elif kernel == 'rbf':
                K[i, j] = np.exp(-gamma * np.linalg.norm(X[i] - X[j]) ** 2)
    
    # Compute the eigenvectors and eigenvalues of the kernel matrix
    eigenvalues, eigenvectors = np.linalg.eigh(K)
    
    # Sort the eigenvectors in decreasing order of their corresponding eigenvalues
    eigenvalues_sorted_indices = np.argsort(eigenvalues)[::-1]
    eigenvectors = eigenvectors[:, eigenvalues_sorted_indices]
    
    # Select the number of dimensions to retain
    k = 3  # Change this value to specify the number of dimensions to retain
    eigenvectors = eigenvectors[:, :k]
    
    # Transform the data into the new space
    X_transformed = np.dot(K, eigenvectors)
    
    return X_transformed

kernel_pca(df_X_train,"rbf", 1)

print(len(df_X_train))
print(len(df_X_test))
print(len(df_y_train))
print(len(df_y_test))

kpca = KernelPCA(n_components=2)
X_train = kpca.fit_transform(df_X_train)
X_test = kpca.transform(df_X_test)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
param_grid = [
    {'C': [ 0.1, 1, 10, 100, 1000], 
     'gamma': [0.0001, 0.001, 0.01, 0.1, 1],
     'kernel': ['rbf']},
    {'C': [0.1, 1, 10, 100, 1000],
     'kernel': ['linear']},
]
grid = GridSearchCV(SVC(), param_grid, verbose=2)
grid.fit(X_train, df_y_train)
classifier = grid

y_pred = grid.predict(X_test)

df1=pd.DataFrame(X_train)
df2=pd.DataFrame(X_test)

pd.plotting.scatter_matrix(df1,color=(blue,red))

print(X_train.shape)
print(X_test.shape)

from matplotlib.colors import ListedColormap
X_set, y_set = X_train, df_y_train
X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),
                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green', 'blue')))

plt.xlim(X1.min(), X1.max())
plt.ylim(X2.min(), X2.max())

for i, j in enumerate(np.unique(y_set)):
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],
                c = ListedColormap(('red', 'green', 'blue'))(i), label = j)

plt.title('SVC (Training set)')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.legend()
plt.show()







df_X_train[0]

kernel_pca(df_X_train,"rbf",1)