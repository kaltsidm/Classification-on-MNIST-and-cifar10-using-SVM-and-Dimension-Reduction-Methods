# -*- coding: utf-8 -*-
"""KLDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DyZPMXaRtztsoXUREy5Ni9fZ3T71g0zM
"""

import numpy as np

def kernel_lda(X, y, kernel, k=None):
    """
    Perform kernel LDA on the given dataset X with labels y and return the top k linear discriminants.
    If k is None, return all the linear discriminants.
    
    Parameters
    ----------
    X : numpy array, shape (n_samples, n_features)
        The dataset to perform kernel LDA on.
    y : numpy array, shape (n_samples,)
        The labels of the samples in the dataset.
    kernel : function
        The kernel function to use. It should take two arguments (x1 and x2) and return a scalar.
    k : int, optional
        The number of top linear discriminants to return.
    
    Returns
    -------
    linear_discriminants : numpy array, shape (n_samples, k)
        The top k linear discriminants of the dataset.
    """
    # Calculate the kernel matrix
    K = np.zeros((X.shape[0], X.shape[0]))
    for i in range(X.shape[0]):
        for j in range(X.shape[0]):
            K[i, j] = kernel(X[i, :], X[j, :])
    
    # Calculate the within-class scatter matrix
    Sw = np.zeros((X.shape[0], X.shape[0]))
    for c in np.unique(y):
        idx = y == c
        Sw += np.dot(K[idx, :][:, idx], np.eye(idx.sum()))
    
    # Calculate the between-class scatter matrix
    mean_vectors = []
    for c in np.unique(y):
        mean_vectors.append(K[y == c, :].mean(axis=0))
    Sb = np.zeros((X.shape[0], X.shape[0]))
    for i, mean_vector in enumerate(mean_vectors):
        n = K[y == i+1, :].shape[0]
        mean_vector = mean_vector.reshape(X.shape[0], 1)
        Sb += n * np.dot(mean_vector, mean_vector.T)
    
    # Calculate the eigenvalues and eigenvectors of the matrix Sw^(-1) * Sb
    eigenvalues, eigenvectors = np.linalg.eig(np.dot(np.linalg.inv(Sw), Sb))
    
    # Sort the eigenvalues and eigenvectors in decreasing order
    idx = np.argsort(-eigenvalues)
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # Select the top k eigenvectors
    if k is not None:
        eigenvectors = eigenvectors[:, :k]
    
    # Project the data onto the eigenvectors
    linear_discriminants = np.dot(K, eigenvectors)
    
    return linear_