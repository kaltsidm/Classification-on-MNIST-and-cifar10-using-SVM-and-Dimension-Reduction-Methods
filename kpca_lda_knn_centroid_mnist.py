# -*- coding: utf-8 -*-
"""kpca-lda-knn-centroid-mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eY__tl3EAXthclJ-sIdbg4H0rbK--biD
"""

#from sklearn. datasets import load_digits 
from sklearn import metrics
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.decomposition import KernelPCA
import math
import seaborn as sns
import numpy as np
import pandas as pd 
import matplotlib. pyplot as plt 
from keras.datasets import mnist
from sklearn import svm
from sklearn.svm import SVC
from sklearn.svm import LinearSVC
import keras
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

(X_train, y_train), (X_test, y_test) = mnist.load_data()

X_train=X_train.flatten().reshape(60000,784)
X_test=X_test.flatten().reshape(10000,784)
X_train=X_train/255.0
X_test=X_test/255.0

X_y_train=pd.DataFrame(X_train)
X_y_train["y"]=y_train
X_y_train=X_y_train.sample(n=12000,replace=False)
X_train=X_y_train.iloc[: , :784]
y_train=X_y_train.iloc[: , 784]

y_train

X_y_test=pd.DataFrame(X_test)
X_y_test["y"]=y_test
X_y_test=X_y_test.sample(n=5000,replace=False)
X_test=X_y_test.iloc[: , :784]
y_test=X_y_test.iloc[: , 784]

#X_train=pd.DataFrame(X_train)
#X_test=pd.DataFrame(X_test)
#X_train=X_train.sample(frac=0.1)
#X_test=X_test.sample(frac=0.1)
#X_train=X_train.to_numpy()
#X_test=X_test.to_numpy()

#y_train=pd.DataFrame(y_train)
#y_test=pd.DataFrame(y_test)
#y_train=y_train.sample(frac=0.1)
#y_test=y_test.sample(frac=0.1)
#y_train=y_train.to_numpy()
#y_test=y_test.to_numpy()

from sklearn.decomposition import KernelPCA
kpca_test = KernelPCA()
kpca_test = kpca_test.fit_transform(X_train)
explained_variance = np.var(kpca_test, axis=0)
explained_variance_ratio = explained_variance / np.sum(explained_variance)

sns.set(style='whitegrid')
plt.plot(np.cumsum(explained_variance_ratio))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance')
display(plt.show())

evr = explained_variance_ratio
cvr = np.cumsum(explained_variance_ratio)

kpca_df = pd.DataFrame()
kpca_df['Cumulative Variance Ratio'] = cvr
kpca_df['Explained Variance Ratio'] = evr
display(kpca_df.head(15))

import time
start=time.time()
kpca = KernelPCA(n_components=100,kernel="rbf",gamma=0.00001)
X_train = kpca.fit_transform(X_train)
X_test = kpca.transform(X_test)
end=time.time()

elapsed_time=end-start 
print(elapsed_time)

clf = LinearDiscriminantAnalysis(n_components=9)
clf.fit(X_train, y_train)

X_train_new=clf.transform(X_train)
X_test_new=clf.transform(X_test)

y_train=y_train.to_numpy()
y_test=y_test.to_numpy()

main_df_after_lda=pd.DataFrame(X_train_new[:,0:2],columns=("1st component","2nd component"))
main_df_after_lda["y"]=y_train

main_df_after_lda

import plotly.express as px
fig = px.scatter(main_df_after_lda, x="1st component", y="2nd component", title='kpca: kernel=rbf,gamma=0.00001 for 12000 train and 5000 test',color="y")
fig.show()

main_df_after_lda=pd.DataFrame(X_test_new[:,0:2],columns=("1st component","2nd component"))
main_df_after_lda["y"]=l
import plotly.express as px
fig = px.scatter(main_df_after_lda, x="1st component", y="2nd component", title='kpca: kernel=rbf,gamma=0.00001',color="y")
fig.show()

# k-NN #
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
knn= KNeighborsClassifier(n_neighbors=2)
model=knn.fit(X_train_new,y_train)
y_predicted=knn.predict(X_test_new)
print("Accuracy: %2f" % metrics.accuracy_score(y_test,y_predicted))
print("Recall: %2f" % metrics.recall_score(y_test,y_predicted, average="macro"))
print("Precision: %2f" % metrics.precision_score(y_test, y_predicted, average="macro"))
print("F1: %2f" % metrics.f1_score(y_test, y_predicted, average="macro"))

A=[]
for k in range(1,51):
    for p in range(1,3):
        for w in ["uniform","distance"]:
            clf=KNeighborsClassifier(n_neighbors=k,weights=w,p=p)
            model=clf.fit(X_train_new,y_train)
            y_predicted=clf.predict(X_test_new)
            f1=metrics.f1_score(y_test, y_predicted, average="macro")
            acc=metrics.accuracy_score(y_test,y_predicted)
            rec=metrics.recall_score(y_test,y_predicted, average="macro")
            pr=metrics.precision_score(y_test, y_predicted, average="macro")
            A.append([k,w,p,f1,acc,rec,pr])

A=pd.DataFrame(A,columns=["k","weights","p","f1","accuracy","recall","precision"])

pltdf=A[A["weights"]=="uniform"][A["p"]==1][["k","accuracy"]]

import plotly.express as px

fig = px.line(pltdf, x="k", y="accuracy", title='accuracy-k for kpca with kernel=rbf,gamma=0.00001')
fig.show()

# nearest Centroid #
from sklearn.neighbors import NearestCentroid
import numpy as np
from sklearn import metrics
clf = NearestCentroid()
model=clf.fit(X_train_new,y_train)
y_predicted=clf.predict(X_test_new)
print("Accuracy: %2f" % metrics.accuracy_score(y_test,y_predicted))
print("Recall: %2f" % metrics.recall_score(y_test,y_predicted, average="macro"))
print("Precision: %2f" % metrics.precision_score(y_test, y_predicted, average="macro"))
print("F1: %2f" % metrics.f1_score(y_test, y_predicted, average="macro"))

